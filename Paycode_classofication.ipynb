{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "import ast\n",
    "import cPickle\n",
    "import datetime\n",
    "\n",
    "mystops = [u'i',u'me',u'my',u'myself',u'we',u'our',u'ours',u'ourselves',u'you',\n",
    "           u'your',u'yours',u'yourself',u'yourselves',u'he',\n",
    "           u'him',u'his',u'himself',u'she',u'her',u'hers',u'herself',u'it',\n",
    "           u'its',u'itself',u'they',u'them',u'their',u'theirs',u'themselves',\n",
    "           u'what',u'which',u'who',u'whom',u'this',u'that',u'these',u'those',\n",
    "           u'am',u'is',u'are',u'was',u'were',u'be',u'been',u'being',u'have',u'has',\n",
    "           u'had',u'having',u'do',u'does',u'did',u'doing',u'a',u'an',u'the',u'and',u'but',\n",
    "           u'if',u'or',u'because',u'as',u'until',u'while',u'of',u'at',u'by',u'for',u'with',\n",
    "           u'about',u'against',u'between',u'into',u'through',u'during',u'before',u'after',u'above',\n",
    "           u'below',u'to',u'from',u'up',u'down',u'in',u'out',u'on',u'off',u'over',u'under',\n",
    "           u'again',u'further',u'then',u'once',u'here',u'there',u'when',u'where',u'why',u'how',\n",
    "           u'all',u'any',u'both',u'each',u'few',u'more',u'most',u'other',u'some',u'such',u'no',\n",
    "           u'nor',u'not',u'only',u'own',u'same',u'so',u'than',u'too',u'very',u's',u't',u'can',\n",
    "           u'will',u'just',u'don',u'should',u'now']\n",
    "### get current file path\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "\n",
    "##set up folders\n",
    "picklesdir = currentdir\n",
    "if not os.path.exists(picklesdir):\n",
    "        os.mkdir(picklesdir)\n",
    "validPicklesdir = picklesdir+'/validPickles'\n",
    "if not os.path.exists(validPicklesdir):\n",
    "        os.mkdir(validPicklesdir)\n",
    "\n",
    "### get lookup file\n",
    "wDict = {}\n",
    "execfile(str(currentdir)+\"/abbr_lookup.py\", wDict)\n",
    "\n",
    "def getTSList(pickelspath = validPicklesdir):\n",
    "    dates = []\n",
    "    for file in sorted(os.listdir(pickelspath)):\n",
    "        if fnmatch.fnmatch(file, '*.pkl'):\n",
    "            dates.append(file.split('-')[1].split('.')[0])            \n",
    "    # filter unique dates and sort them in ascending order\n",
    "    return sorted(list(set(dates)))   \n",
    "\n",
    "import cPickle\n",
    "def pickle_model(filename,model):\n",
    "    with open(str(picklesdir+'/'+filename) + '.pkl', 'wb') as fid:\n",
    "        cPickle.dump(model, fid)\n",
    "    print \"Pickled Model at\" + str(picklesdir)\n",
    "\n",
    "def read_pickle(filename,pickelspath = validPicklesdir):\n",
    "    with open(pickelspath + '/' + str(filename) + '.pkl', 'rb') as fid:\n",
    "        gnb_loaded = cPickle.load(fid)\n",
    "    return gnb_loaded\n",
    "\n",
    "\n",
    "def load_pickles(TS):\n",
    "    svmModelL1 = read_pickle('paycode_L1_SVM_classifier-'+TS)\n",
    "    svmModelL2 = read_pickle('paycode_L2_SVM_classifier-'+TS)\n",
    "    vectorizerL1 = read_pickle('paycode_L1_tfidf_vectorizer-'+TS)\n",
    "    vectorizerL2 = read_pickle('paycode_L2_tfidf_vectorizer-'+TS)\n",
    "    return (svmModelL1,svmModelL2,vectorizerL1,vectorizerL2)\n",
    "\n",
    "def mark_pickle_TS_valid(TS):\n",
    "    frompath = picklesdir+'/paycode_L1_SVM_classifier-'+TS+'.pkl'\n",
    "    topath = validPicklesdir+'/paycode_L1_SVM_classifier-'+TS+'.pkl'\n",
    "    os.rename(frompath,topath)\n",
    "    \n",
    "    frompath = picklesdir+'/paycode_L2_SVM_classifier-'+TS+'.pkl'\n",
    "    topath = validPicklesdir+'/paycode_L2_SVM_classifier-'+TS+'.pkl'\n",
    "    os.rename(frompath,topath)\n",
    "    \n",
    "    frompath = picklesdir+'/paycode_L1_tfidf_vectorizer-'+TS+'.pkl'\n",
    "    topath = validPicklesdir+'/paycode_L1_tfidf_vectorizer-'+TS+'.pkl'\n",
    "    os.rename(frompath,topath)\n",
    "    \n",
    "    frompath = picklesdir+'/paycode_L2_tfidf_vectorizer-'+TS+'.pkl'\n",
    "    topath = validPicklesdir+'/paycode_L2_tfidf_vectorizer-'+TS+'.pkl'    \n",
    "    os.rename(frompath,topath)\n",
    "\n",
    "# get the last element, it will be the latest date as teh list is sorted\n",
    "#pickles = load_pickles(getTSList(currentdir).pop())\n",
    "pickles = load_pickles(getTSList().pop())\n",
    "#####################################################################################################\n",
    "## Text Pre-processing functions\n",
    "def cleanText(s,keepPunct):\n",
    "    import string\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    punctuation = ''.join(list(set(string.punctuation) - set(keepPunct)))\n",
    "    table = string.maketrans(\"\",\"\")\n",
    "    s = s.translate(table,punctuation)\n",
    "    s = ' '.join(''.join(i for i in token if not i.isdigit()) for token in word_tokenize(s))\n",
    "    return s.strip()\n",
    "\n",
    "def multiwordReplace(text, wDict):\n",
    "    '''\n",
    "    Replace words in a text that match a key in replace_dict\n",
    "    with the associated value, return the modified text.\n",
    "    Only whole words are replaced.\n",
    "    '''\n",
    "    rc = re.compile(r\"[A-Za-z_]\\w*\")\n",
    "    def translate(match):\n",
    "        word = match.group(0)\n",
    "        return wDict.get(word, word)\n",
    "    return rc.sub(translate, text)\n",
    "\n",
    "def cleanPayCd(pcname,pcdesc,keepPunct,wDict):\n",
    "    #mystops = stopwords.words('english')\n",
    "    paycd_text = pcdesc + ' ' + pcname\n",
    "    prepText = ' '.join(token.lower() for token in word_tokenize(paycd_text) if token not in mystops)\n",
    "    prepText = cleanText(prepText,keepPunct = keepPunct)\n",
    "    prepText = ' '.join(multiwordReplace(t,wDict) for t in prepText.split(' '))\n",
    "    return prepText\n",
    "\n",
    "def preprocess(pcData,keepPunct=[],wrdDict=wDict):\n",
    "    pcData['PAYCD_TEXT'] = pcData['PAYCD_SHRT_DSC'] + ' ' + pcData['PAYCD_NM']\n",
    "    paycd_text = list(pcData['PAYCD_TEXT'])\n",
    "    prepText = [' '.join(token.lower() for token in word_tokenize(text) if token not in mystops) \n",
    "                for text in paycd_text]\n",
    "    prepText = [cleanText(text,keepPunct = keepPunct) for text in prepText]\n",
    "    prepText = [' '.join(multiwordReplace(t,wrdDict) for t in text.split(' ')) for text in prepText]\n",
    "    pcData.drop('PAYCD_TEXT',axis=1,inplace=True)\n",
    "    pcData['PAYCD_TEXT'] = pd.Series(prepText, index=pcData.index)\n",
    "    print \"--------------------------------------------------------------------- \\n\"\n",
    "    print \"Pre-Processing Completed for \" + str(pcData.shape[0]) + \" PayCode records...\"\n",
    "    print \"--------------------------------------------------------------------- \\n\"\n",
    "    return pcData\n",
    "####################################################################################################\n",
    "### predict methods reading stored pickles of Models and Vectorizers\n",
    "def predictL1(model,vectorizer,pcdesc,pcname,wDict,threshold=0.8):\n",
    "    paycodeText= cleanPayCd(pcname,pcdesc,keepPunct=['-','$'],wDict=wDict)\n",
    "    X_pred = vectorizer.transform(np.array([paycodeText]))\n",
    "    pclasses = model.classes_\n",
    "    y_predicted = model.predict(X_pred[0])[0]\n",
    "    yprob = model.predict_proba(X_pred[0])[0]\n",
    "    yprob = [round(x,3) for x in yprob]\n",
    "    prediction = [(x,y) for (y,x) in sorted(zip(yprob,pclasses),reverse=True)]\n",
    "    prediction = [pred[1][[x[0] for x in pred[1]].index(pred[0])] for pred in zip([y_predicted],[prediction])]\n",
    "    if prediction[0][1] <= threshold:\n",
    "        return ((\"Other\",1),paycodeText)\n",
    "    else:\n",
    "        return (prediction[0],paycodeText)\n",
    "\n",
    "\n",
    "def predictL2(modelL2,vectorizer,modelL1,paycodeL1):\n",
    "    ### filter where predictions are necessary\n",
    "    if paycodeL1[0][0] == \"Other\":\n",
    "        prediction = (\"Other\",1)\n",
    "        return prediction\n",
    "    elif paycodeL1[0][0] == \"Regular\":\n",
    "        prediction = (\"Regular\",1)\n",
    "        return prediction\n",
    "    elif paycodeL1[0][0] == \"Premium Time\":\n",
    "        prediction = (\"Doubletime\",1)\n",
    "        return prediction\n",
    "    else:\n",
    "        L2Original = vectorizer.transform(np.array([paycodeL1[1]]))\n",
    "        L2Classes = [\"Absence\",\"Overtime\"]\n",
    "        dummyL1 = sp.csr_matrix(pd.get_dummies(pd.Series(list(L2Classes)))[paycodeL1[0][0]].as_matrix())\n",
    "        X_test = sp.hstack((L2Original,dummyL1),format='csr')\n",
    "        yprob = [round(x,3) for x in modelL2.predict_proba(X_test[0])[0]]\n",
    "        prediction = [(x,y) for (y,x) in sorted(zip(yprob,modelL2.classes_),reverse=True)]\n",
    "        prediction = [pred[1][[x[0] for x in pred[1]].index(pred[0])] for pred in zip([modelL2.predict(X_test[0])[0]],[prediction])]\n",
    "        return prediction[0]\n",
    "\n",
    "### un-pickle saved components\n",
    "#svmModelL1 = read_pickle('paycode_L1_SVM_classifier')\n",
    "#svmModelL2 = read_pickle('paycode_L2_SVM_classifier')\n",
    "#vectorizerL1 = read_pickle('paycode_L1_tfidf_vectorizer')\n",
    "#vectorizerL2 = read_pickle('paycode_L2_tfidf_vectorizer')\n",
    "\n",
    "### wrapper function for prediction (both L1 and L2)\n",
    "def classifyPC(pcname,pcdesc):\n",
    "    L1pred = predictL1(model=pickles[0],vectorizer=pickles[2],pcdesc=pcdesc,pcname=pcname,wDict=wDict)\n",
    "    L2pred = predictL2(modelL2=pickles[1],vectorizer=pickles[3],modelL1=pickles[0],paycodeL1=L1pred)\n",
    "    return {'paycodeGroupCode':L1pred[0][0],'paycodeSubGroupCode':L2pred[0]}\n",
    "\n",
    "\n",
    "### wrapper function to predict for a DataFrame of PayCodes --uses classifyPC defined above\n",
    "def batchPCpredict(store_location,filename,input_path=currentdir):\n",
    "    if filename.endswith('.csv'):\n",
    "        data = pd.read_csv(input_path + '/' + filename,header=0)\n",
    "    else:\n",
    "        data = pd.read_csv(input_path + '/' + filename + '.csv',header=0)\n",
    "    data['groupCode']=data.apply(lambda x:classifyPC(str(x['name']),\n",
    "                                                  str(x['shortDescription']))['paycodeGroupCode'],axis=1)\n",
    "    data['subGroupCode']=data.apply(lambda x:classifyPC(str(x['name']),\n",
    "                                                  str(x['shortDescription']))['paycodeSubGroupCode'],axis=1)\n",
    "    if store_location == '':\n",
    "        store_location = input_path\n",
    "    if filename.endswith('.csv'):\n",
    "        data.to_csv(store_location + '/' + str(filename)[:-4] + '_predicted.csv',index=False)\n",
    "        print  str(filename)[:-4] + '_predicted.csv' + ' saved at ' + store_location + '/'\n",
    "    else:\n",
    "        data.to_csv(store_location + '/' + str(filename) + '_predicted.csv',index=False)\n",
    "        print  str(filename) + '_predicted.csv' + ' saved at ' + store_location + '/'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
